{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b45d3dd-7fda-4ec4-ba5a-2e0108b162e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SRINADH\\AppData\\Local\\Temp\\ipykernel_12656\\1662815981.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e176dade-259d-4776-9bd7-04f3815927ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "y = np.array([1, 3, 2, 5, 7, 8, 8, 9, 10, 12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d693e8a-adb5-44d4-8332-d4770155308b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytic Solution:\n",
      "b0: 1.2363636363636372\n",
      "b1: 1.1696969696969695\n",
      "SSE: 5.624242424242422\n",
      "Rsquare: 0.952538038613988\n"
     ]
    }
   ],
   "source": [
    "def linear_regression_analytic(x, y):\n",
    "    n = len(x)\n",
    "    x_mean = np.mean(x)\n",
    "    y_mean = np.mean(y)\n",
    "    xy_mean = np.mean(x*y)\n",
    "    x_squared_mean = np.mean(x**2)\n",
    "    b1 = (xy_mean - x_mean*y_mean) / (x_squared_mean - x_mean**2)\n",
    "    b0 = y_mean - b1*x_mean\n",
    "    y_pred = b0 + b1*x\n",
    "    sse = np.sum((y - y_pred)**2)\n",
    "    ss_total = np.sum((y - y_mean)**2)\n",
    "    r_squared = 1 - (sse / ss_total)\n",
    "    return b0, b1, sse, r_squared\n",
    "b0_analytic, b1_analytic, sse_analytic, r_squared_analytic = linear_regression_analytic(x, y)\n",
    "print(\"Analytic Solution:\")\n",
    "print(\"b0:\", b0_analytic)\n",
    "print(\"b1:\", b1_analytic)\n",
    "print(\"SSE:\", sse_analytic)\n",
    "print(\"Rsquare:\", r_squared_analytic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b922f375-b17d-47b8-b530-81e04881ff21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Residual Errors: 2.220446049250313e-15\n"
     ]
    }
   ],
   "source": [
    "def sum_residual_errors(x, y, b0, b1):\n",
    "    y_pred = b0 + b1 * x\n",
    "    residual_errors = y - y_pred\n",
    "    sum_residual = np.sum(residual_errors)\n",
    "    return sum_residual\n",
    "b0_analytic, b1_analytic, _, _ = linear_regression_analytic(x, y)\n",
    "sum_residual_analytic = sum_residual_errors(x, y, b0_analytic, b1_analytic)\n",
    "print(\"Sum of Residual Errors:\", sum_residual_analytic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0049e778-edf6-4c72-9f97-43d98aa8e4ea",
   "metadata": {},
   "source": [
    "# Full Batch Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1005f0aa-a03e-4991-ae89-9c313f51167b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full-batch Gradient Descent:\n",
      "b0: 1.230898466943318\n",
      "b1: 1.170568526128318\n",
      "SSE: 5.624329890820989\n"
     ]
    }
   ],
   "source": [
    "def full_batch_gradient_descent(x, y, learning_rate=0.01, epochs=1000, tolerance=1e-6):\n",
    "    n = len(x)\n",
    "    b0 = 0\n",
    "    b1 = 0\n",
    "    prev_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        y_pred = b0 + b1*x\n",
    "\n",
    "        gradient_b0 = (-2/n) * np.sum(y - y_pred)\n",
    "        gradient_b1 = (-2/n) * np.sum((y - y_pred) * x)\n",
    "\n",
    "        b0 -= learning_rate * gradient_b0\n",
    "        b1 -= learning_rate * gradient_b1\n",
    "\n",
    "        loss = np.sum((y - y_pred)**2)\n",
    "\n",
    "        if abs(prev_loss - loss) < tolerance:\n",
    "            break\n",
    "\n",
    "        prev_loss = loss\n",
    "\n",
    "    return b0, b1, loss\n",
    "\n",
    "b0_full, b1_full, sse_full = full_batch_gradient_descent(x, y)\n",
    "print(\"\\nFull-batch Gradient Descent:\")\n",
    "print(\"b0:\", b0_full)\n",
    "print(\"b1:\", b1_full)\n",
    "print(\"SSE:\", sse_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a09857-5ba8-4915-ba26-ee2fde462129",
   "metadata": {},
   "source": [
    "# Stochastic Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10a4e1bc-ba10-4d5c-a6e6-aec6f4a1ae58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stochastic Gradient Descent:\n",
      "b0 1.2694049165731163\n",
      "b1: 1.2319087851931128\n",
      "SSE: 6.923198255142552\n"
     ]
    }
   ],
   "source": [
    "def stochastic_gradient_descent(x, y, learning_rate=0.01, epochs=1000, tolerance=1e-6):\n",
    "    n = len(x)\n",
    "    b0 = 0\n",
    "    b1 = 0\n",
    "    prev_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        indices = np.random.permutation(n)\n",
    "        x_shuffled = x[indices]\n",
    "        y_shuffled = y[indices]\n",
    "\n",
    "        for i in range(n):\n",
    "            y_pred = b0 + b1*x_shuffled[i]\n",
    "\n",
    "            gradient_b0 = -2 * (y_shuffled[i] - y_pred)\n",
    "            gradient_b1 = -2 * (y_shuffled[i] - y_pred) * x_shuffled[i]\n",
    "\n",
    "            b0 -= learning_rate * gradient_b0\n",
    "            b1 -= learning_rate * gradient_b1\n",
    "\n",
    "        y_pred = b0 + b1*x\n",
    "        loss = np.sum((y - y_pred)**2)\n",
    "\n",
    "        if abs(prev_loss - loss) < tolerance:\n",
    "            break\n",
    "\n",
    "        prev_loss = loss\n",
    "\n",
    "    return b0, b1, loss\n",
    "\n",
    "\n",
    "b0_stochastic, b1_stochastic, sse_stochastic = stochastic_gradient_descent(x, y)\n",
    "print(\"\\nStochastic Gradient Descent:\")\n",
    "print(\"b0\", b0_stochastic)\n",
    "print(\"b1:\", b1_stochastic)\n",
    "print(\"SSE:\", sse_stochastic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae6d078-e556-4229-bcd8-cbb310883b22",
   "metadata": {},
   "source": [
    "#2 Download Boston Housing Rate Dataset. Analyse the input attributes and find out the attribute that best follow the linear relationship with the output price. Implement both the analytic formulation and gradient descent (Full-batch, stochastic) on LMS loss formulation to compute the coefficients of regression matrix and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f8cdad9-9cc1-4947-968a-f329fa56b48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients using Analytic Formulation: [44459.72916908 41933.84939381]\n",
      "Coefficients using Full-batch Gradient Descent: [39148.47787113 43047.96802282]\n",
      "Coefficients using Stochastic Gradient Descent: [48526.50208942 61525.7562141 ]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "housing_data = pd.read_csv(\"housing.csv\")\n",
    "\n",
    "selected_attribute = 'median_income'\n",
    "X = housing_data[selected_attribute].values.reshape(-1, 1)\n",
    "y = housing_data['median_house_value'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_with_intercept = np.c_[np.ones(X_train.shape[0]), X_train]\n",
    "X_test_with_intercept = np.c_[np.ones(X_test.shape[0]), X_test]\n",
    "\n",
    "theta_analytic = np.linalg.inv(X_train_with_intercept.T.dot(X_train_with_intercept)).dot(X_train_with_intercept.T).dot(y_train)\n",
    "print(\"Coefficients using Analytic Formulation:\", theta_analytic)\n",
    "\n",
    "def full_batch_gradient_descent(X, y, learning_rate, num_iterations):\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    for _ in range(num_iterations):\n",
    "        y_pred = X.dot(theta)\n",
    "        theta -= (1/len(y)) * learning_rate * X.T.dot(y_pred - y)\n",
    "    return theta\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_iterations = 1000\n",
    "theta_full_batch = full_batch_gradient_descent(X_train_with_intercept, y_train, learning_rate, num_iterations)\n",
    "print(\"Coefficients using Full-batch Gradient Descent:\", theta_full_batch)\n",
    "\n",
    "def stochastic_gradient_descent(X, y, learning_rate, num_iterations):\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    m = len(y)  # Number of training examples\n",
    "\n",
    "    # Shuffle the training data once before training\n",
    "    shuffle_index = np.random.permutation(len(y))\n",
    "    X_shuffled = X[shuffle_index]\n",
    "    y_shuffled = y[shuffle_index]\n",
    "\n",
    "    # SGD loop\n",
    "    for _ in range(num_iterations):\n",
    "        for xi, yi in zip(X_shuffled, y_shuffled):\n",
    "            y_pred = np.dot(xi, theta)\n",
    "            gradient = xi * (y_pred - yi)\n",
    "            theta -= learning_rate * gradient\n",
    "\n",
    "    return theta\n",
    "\n",
    "theta_stochastic = stochastic_gradient_descent(X_train_with_intercept, y_train, learning_rate, num_iterations)\n",
    "print(\"Coefficients using Stochastic Gradient Descent:\", theta_stochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de904349-bd0e-40e4-ba17-93598ea02809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSE and R-squared value:\n",
      "Analytic Formulation: SSE = 29272299281848.184 , R-squared = 0.4588591890384667\n",
      "Full-batch Gradient Descent: SSE = 29321631561932.883 , R-squared = 0.4579472104543938\n",
      "Stochastic Gradient Descent: SSE = 60846888763198.53 , R-squared = -0.12484278781002356\n"
     ]
    }
   ],
   "source": [
    "y_pred_analytic = X_test_with_intercept.dot(theta_analytic)\n",
    "y_pred_full_batch = X_test_with_intercept.dot(theta_full_batch)\n",
    "y_pred_stochastic = X_test_with_intercept.dot(theta_stochastic)\n",
    "\n",
    "# Calculate SSE\n",
    "SSE_analytic = np.sum((y_test - y_pred_analytic) ** 2)\n",
    "SSE_full_batch = np.sum((y_test - y_pred_full_batch) ** 2)\n",
    "SSE_stochastic = np.sum((y_test - y_pred_stochastic) ** 2)\n",
    "\n",
    "# Calculate total sum of squares (SST)\n",
    "mean_y = np.mean(y_test)\n",
    "SST = np.sum((y_test - mean_y) ** 2)\n",
    "\n",
    "# Calculate R-squared\n",
    "R_squared_analytic = 1 - (SSE_analytic / SST)\n",
    "R_squared_full_batch = 1 - (SSE_full_batch / SST)\n",
    "R_squared_stochastic = 1 - (SSE_stochastic / SST)\n",
    "\n",
    "print(\"SSE and R-squared value:\")\n",
    "print(\"Analytic Formulation: SSE =\", SSE_analytic, \", R-squared =\", R_squared_analytic)\n",
    "print(\"Full-batch Gradient Descent: SSE =\", SSE_full_batch, \", R-squared =\", R_squared_full_batch)\n",
    "print(\"Stochastic Gradient Descent: SSE =\", SSE_stochastic, \", R-squared =\", R_squared_stochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e2a13c-a1da-40f2-bf1f-58bf85e2423f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
